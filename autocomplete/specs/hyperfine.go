// Code generated by autocomplete/extract/extract.ts. DO NOT EDIT.
//
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT License.

package specs

import (
	"github.com/microsoft/clac/autocomplete/model"
)

func init() {
	Specs["hyperfine"] = model.Subcommand{
		Name:        []string{"hyperfine"},
		Description: `A command-line benchmarking tool`,
		Args: []model.Arg{{
			Name:        "CMD",
			Description: `Command to benchmark`,
			IsCommand:   true,
		}},
		Options: []model.Option{{
			Name:        []string{"--warmup", "-w"},
			Description: `Perform warmupruns (number) before the actual benchmarking starts`,
			Args: []model.Arg{{
				Name: "NUM",
			}},
		}, {
			Name:        []string{"--min-runs", "-m"},
			Description: `Perform at least NUM runs for each command`,
			Args: []model.Arg{{
				Name: "NUM",
			}},
		}, {
			Name:        []string{"--max-runs", "-M"},
			Description: `Perform at most NUM runs for each command. Default: no limit`,
			Args: []model.Arg{{
				Name: "NUM",
			}},
		}, {
			Name:        []string{"--runs", "-r"},
			Description: `Perform exactly NUM runs for each command`,
			Args: []model.Arg{{
				Name: "NUM",
			}},
		}, {
			Name:        []string{"--setup", "-s"},
			Description: `Execute cmd once before each set of timing runs`,
			Args: []model.Arg{{
				Name:      "CMD",
				IsCommand: true,
			}},
		}, {
			Name:        []string{"--prepare", "-p"},
			Description: `Execute cmd before each timing run. This is useful for clearing disk caches, for example`,
			Args: []model.Arg{{
				Name:      "CMD ...",
				IsCommand: true,
			}},
		}, {
			Name:        []string{"--cleanup", "-c"},
			Description: `Execute cmd after the completion of all benchmarking runs for each individual command to be benchmarked`,
			Args: []model.Arg{{
				Name:      "CMD",
				IsCommand: true,
			}},
		}, {
			Name:        []string{"--parameter-scan", "-P"},
			Description: `Perform benchmark runs for each value in the range min..max`,
			Args: []model.Arg{{
				Name: "VAR",
			}, {
				Name: "MIN",
			}, {
				Name: "MAX",
			}},
		}, {
			Name:        []string{"--parameter-step-size", "-D"},
			Description: `This argument requires --parameter-scan to be specified as well. Traverse the range min..max in steps of delta`,
			Args: []model.Arg{{
				Name: "delta",
			}},
		}, {
			Name:        []string{"--parameter-list", "-L"},
			Description: `Perform benchmark runs for each value in the comma-separated list of values`,
			Args: []model.Arg{{
				Name: "VAR",
			}, {
				Name: "VALS",
			}},
		}, {
			Name:        []string{"--style"},
			Description: `Set output style type`,
			Args: []model.Arg{{
				Name: "STYLE",
				Suggestions: []model.Suggestion{{
					Name:        []string{`basic`},
					Description: `Disable output coloring and interactive elements`,
				}, {
					Name:        []string{`full`},
					Description: `Enable all effects even if no interactive terminal was detected`,
				}, {
					Name:        []string{`nocolor`},
					Description: `Keep the interactive output without any colors`,
				}, {
					Name:        []string{`color`},
					Description: `Keep the colors without any interactive output`,
				}, {
					Name:        []string{`none`},
					Description: `Disable all the output of the tool`,
				}},
			}},
		}, {
			Name:        []string{"--shell", "-S"},
			Description: `Set the shell to use for executing benchmarked commands`,
			Args: []model.Arg{{
				Name: "SHELL",
				Suggestions: []model.Suggestion{{
					Name:        []string{`bash`},
					Description: `Use bash as the shell`,
				}, {
					Name:        []string{`zsh`},
					Description: `Use zsh as the shell`,
				}, {
					Name:        []string{`sh`},
					Description: `Use sh as the shell`,
				}, {
					Name:        []string{`fish`},
					Description: `Use fish as the shell`,
				}, {
					Name:        []string{`pwsh`},
					Description: `Use pwsh as the shell`,
				}, {
					Name:        []string{`powershell`},
					Description: `Use powershell as the shell`,
				}},
			}},
		}, {
			Name:        []string{"--ignore-failure", "-i"},
			Description: `Ignore non-zero exit codes of the benchmarked commands`,
		}, {
			Name:        []string{"--time-unit", "-u"},
			Description: `Set the time unit to use for the benchmark results`,
			Args: []model.Arg{{
				Name:        "UNIT",
				Suggestions: []model.Suggestion{{Name: []string{`millisecond`}}, {Name: []string{`second`}}},
			}},
		}, {
			Name:        []string{"--export-asciidoc"},
			Description: `Export the timing summary statistics as an AsciiDoc table to the given file`,
			Args: []model.Arg{{
				Templates: []model.Template{model.TemplateFilepaths},
				Name:      "FILE",
			}},
		}, {
			Name:        []string{"--export-csv"},
			Description: `Export the timing summary statistics as CSV to the given file`,
			Args: []model.Arg{{
				Templates: []model.Template{model.TemplateFilepaths},
				Name:      "FILE",
			}},
		}, {
			Name:        []string{"--export-json"},
			Description: `Export the timing summary statistics and timings of individual runs as JSON to the given file`,
			Args: []model.Arg{{
				Templates: []model.Template{model.TemplateFilepaths},
				Name:      "FILE",
			}},
		}, {
			Name:        []string{"--export-markdown"},
			Description: `Export the timing summary statistics as a Markdown table to the given file`,
			Args: []model.Arg{{
				Templates: []model.Template{model.TemplateFilepaths},
				Name:      "FILE",
			}},
		}, {
			Name:        []string{"--show-output"},
			Description: `Print the stdout and stderr of the benchmark instead of suppressing it`,
		}, {
			Name:        []string{"--command-name"},
			Description: `Identify a command with the given name`,
			Args: []model.Arg{{
				Name: "NAME",
			}},
		}, {
			Name:        []string{"--help"},
			Description: `Prints help message`,
		}, {
			Name:        []string{"--version"},
			Description: `Shows version information`,
		}},
	}
}
